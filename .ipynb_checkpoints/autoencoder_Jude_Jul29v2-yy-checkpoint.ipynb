{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83df4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd33dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ieee-fraud-detection/train_transaction.csv\")\n",
    "\n",
    "# derive “day” from TransactionDT, then drop the raw column\n",
    "df[\"day\"] = (df[\"TransactionDT\"] // (3600 * 24)).astype(int)\n",
    "df.drop(\"TransactionDT\", axis=1, inplace=True)\n",
    "\n",
    "# drop TransactionID, as it is not useful for modeling\n",
    "df.drop(\"TransactionID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a435cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b81da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing % for all columns\n",
    "nulls = df.isna().mean() * 100\n",
    "\n",
    "# find columns with more than 80% missing values\n",
    "cols_80 = nulls[nulls >= 80].index.tolist()\n",
    "\n",
    "# and drop them!\n",
    "df.drop(columns=cols_80, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b01e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# numeric imputation (median) – exclude the target “isFraud”\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"isFraud\")\n",
    "\n",
    "# among num_cols, find columns with nans that need to be imputed\n",
    "nan_cols = [c for c in num_cols if df[c].isna().any()]\n",
    "\n",
    "# exclude the categorical columns card2, card3, card5, addr1, addr2\n",
    "cat_cols = [\"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\"]\n",
    "nan_cols = [c for c in nan_cols if c not in cat_cols]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[nan_cols] = imputer.fit_transform(df[nan_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26124b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for remaining categoricals, one‐hot encode small‐cardinaliy ones else drop them\n",
    "cat_cols_rem = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# include the cat_cols that were excluded earlier\n",
    "cat_cols_rem.extend(cat_cols)\n",
    "\n",
    "# e.g. “ProductCD”, “MISSING” placeholders, etc.\n",
    "for c in cat_cols_rem:\n",
    "    n_uniq = df[c].nunique()\n",
    "    if n_uniq <= 10:\n",
    "        # ------------------ yy -----------------------\n",
    "        if c[0] == 'M':\n",
    "            if c[1] != 4:\n",
    "                df[c] = df[c].map({'T': 1, 'F': 0})\n",
    "            else:\n",
    "                df[c] = df[c].map({'M0': 0, 'M1': 1, 'M2':2})\n",
    "        else:\n",
    "        # ---------------------------------------------\n",
    "            dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "            df = pd.concat([df.drop(c, axis=1), dummies], axis=1)\n",
    "    else:\n",
    "        df.drop(columns=c, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40bb5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all bool columns in df\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "# cast them to int (True→1, False→0)\n",
    "df[bool_cols] = df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b38d6a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>dist1</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>...</th>\n",
       "      <th>ProductCD_H</th>\n",
       "      <th>ProductCD_R</th>\n",
       "      <th>ProductCD_S</th>\n",
       "      <th>ProductCD_W</th>\n",
       "      <th>card4_discover</th>\n",
       "      <th>card4_mastercard</th>\n",
       "      <th>card4_visa</th>\n",
       "      <th>card6_credit</th>\n",
       "      <th>card6_debit</th>\n",
       "      <th>card6_debit or credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>13926</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2755</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>287.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18132</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4497</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 338 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   isFraud  TransactionAmt  card1  dist1   C1   C2   C3   C4   C5   C6  ...  \\\n",
       "0        0            68.5  13926   19.0  1.0  1.0  0.0  0.0  0.0  1.0  ...   \n",
       "1        0            29.0   2755    8.0  1.0  1.0  0.0  0.0  0.0  1.0  ...   \n",
       "2        0            59.0   4663  287.0  1.0  1.0  0.0  0.0  0.0  1.0  ...   \n",
       "3        0            50.0  18132    8.0  2.0  5.0  0.0  0.0  0.0  4.0  ...   \n",
       "4        0            50.0   4497    8.0  1.0  1.0  0.0  0.0  0.0  1.0  ...   \n",
       "\n",
       "   ProductCD_H  ProductCD_R  ProductCD_S  ProductCD_W  card4_discover  \\\n",
       "0            0            0            0            1               1   \n",
       "1            0            0            0            1               0   \n",
       "2            0            0            0            1               0   \n",
       "3            0            0            0            1               0   \n",
       "4            1            0            0            0               0   \n",
       "\n",
       "   card4_mastercard  card4_visa  card6_credit  card6_debit  \\\n",
       "0                 0           0             1            0   \n",
       "1                 1           0             1            0   \n",
       "2                 0           1             0            1   \n",
       "3                 1           0             0            1   \n",
       "4                 1           0             1            0   \n",
       "\n",
       "   card6_debit or credit  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "\n",
       "[5 rows x 338 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8eed19-1e90-4ebb-8615-fb4be3925b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0d4311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Training on normals only: (364720, 337)\n",
      "→ CV set (normals+fraud): (91181, 337)\n"
     ]
    }
   ],
   "source": [
    "# Create training and CV sets\n",
    "# all non‐fraud examples\n",
    "df_norm = df[df.isFraud == 0].copy()\n",
    "# all fraud examples\n",
    "df_fraud = df[df.isFraud == 1].copy()\n",
    "\n",
    "# --------------------- yy -----------------------\n",
    "# hold out 20% of normals for test\n",
    "# hold out 20% of training for cross validation, leaving test for assessing model performance only, not model selection or tuning.\n",
    "norm_train, norm_test = train_test_split(df_norm, test_size=0.2, random_state=42)\n",
    "norm_train, norm_cv = train_test_split(norm_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# test set = held‐out test + all frauds\n",
    "df_test = pd.concat([norm_test, df_fraud], axis=0)\n",
    "y_test = df_test[\"isFraud\"].values\n",
    "# SHOULD THE LABEL IN df_test BE DROPPED AS WELL?\n",
    "\n",
    "# drop labels for modeling\n",
    "X_train = norm_train.drop(\"isFraud\", axis=1)\n",
    "X_cv    = norm_cv.drop(\"isFraud\", axis=1)\n",
    "\n",
    "print(f'train set(64% normal data): {X_train.shape}')\n",
    "print(f'cv set(16% normal data): {X_cv.shape}')\n",
    "print(f'test set(20% normal data + fraud): {df_test.shape}')\n",
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c265308-d26b-481f-b734-66b506a767f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set(64% normal data): (364720, 337)\n",
      "cv set(16% normal data): (91181, 337)\n",
      "test set(20% normal data + fraud): (134639, 338)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d7cbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify one-hot columns (all values are 0 or 1)\n",
    "one_hot_cols = [col for col in X_train.columns if set(X_train[col].unique()) <= {0, 1}]\n",
    "non_one_hot_cols = [col for col in X_train.columns if col not in one_hot_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3a6cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_cv_scaled    = X_cv.copy()\n",
    "\n",
    "X_train_scaled[non_one_hot_cols] = scaler.fit_transform(X_train[non_one_hot_cols])\n",
    "X_cv_scaled[non_one_hot_cols]    = scaler.transform(X_cv[non_one_hot_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f9baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train_scaled.values\n",
    "X_cv_final    = X_cv_scaled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6321be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_train = torch.FloatTensor(X_train_final)\n",
    "x_valid = torch.FloatTensor(X_cv_final)\n",
    "y_valid = torch.FloatTensor(y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9baaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output must match the input for autoencoders\n",
    "\n",
    "class FraudDatasetUnsupervised(Dataset):\n",
    "    \n",
    "    def __init__(self, x,output=True):\n",
    "        'Initialization'\n",
    "        self.x = x\n",
    "        self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample index\n",
    "        item = self.x[index]\n",
    "        if self.output:\n",
    "            return item, item\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b3d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = FraudDatasetUnsupervised(x_train)\n",
    "valid_set = FraudDatasetUnsupervised(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82ad6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Pytorch loaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(valid_set,   batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a8f9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DropAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size_1, intermediate_size_2, code_size, dropout_rate=0.2):\n",
    "        super(DropAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_size, intermediate_size_1)\n",
    "        self.fc2 = nn.Linear(intermediate_size_1, intermediate_size_2)\n",
    "        self.fc3 = nn.Linear(intermediate_size_2, code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(code_size, intermediate_size_2)\n",
    "        self.fc5 = nn.Linear(intermediate_size_2, intermediate_size_1)\n",
    "        self.fc6 = nn.Linear(intermediate_size_1, input_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder with dropout noise\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        code = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.relu(self.fc4(code))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc6(x)  # Linear activation\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a15fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9665bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sample_mse(model, generator):\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    batch_losses = []\n",
    "    \n",
    "    for x_batch, y_batch in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        loss_app = list(torch.mean(loss,axis=1).detach().cpu().numpy())\n",
    "        batch_losses.extend(loss_app)\n",
    "    \n",
    "    return batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77e91dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "model = DropAutoencoder(x_train.shape[1], 128, 64, 16, dropout_rate=0.2)\n",
    "losses = per_sample_mse(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98a60d92-68f7-40b7-ba44-cd101a0225c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DropAutoencoder(\n",
      "  (fc1): Linear(in_features=338, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc6): Linear(in_features=128, out_features=338, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#yy\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "721b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.104663715, 3.11748, 0.12473496, 0.24563716, 0.9639551]\n",
      "2.0011084\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66da7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,generator,criterion):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        batch_losses.append(loss.item())\n",
    "    mean_loss = np.mean(batch_losses)    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04b30bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=3, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf\n",
    "    \n",
    "    def continue_training(self,current_score):\n",
    "        if self.best_score > current_score:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(\"New best score:\", current_score)\n",
    "        else:\n",
    "            self.counter+=1\n",
    "            if self.verbose:\n",
    "                print(self.counter, \" iterations since best score.\")\n",
    "                \n",
    "        return self.counter <= self.patience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4465904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,training_generator,valid_generator,optimizer,criterion,max_epochs=100,apply_early_stopping=True,patience=3,verbose=False):\n",
    "    #Setting the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    if apply_early_stopping:\n",
    "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
    "    \n",
    "    all_train_losses = []\n",
    "    all_valid_losses = []\n",
    "    \n",
    "    #Training loop\n",
    "    start_time=time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss=[]\n",
    "        for x_batch, y_batch in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(x_batch)\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        #showing last training loss after each epoch\n",
    "        all_train_losses.append(np.mean(train_loss))\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
    "        #evaluating the model on the test set after each epoch    \n",
    "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
    "        all_valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print('valid loss: {}'.format(valid_loss))\n",
    "        if apply_early_stopping:\n",
    "            if not early_stopping.continue_training(valid_loss):\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "    training_execution_time=time.time()-start_time\n",
    "    return model,training_execution_time,all_train_losses,all_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d616349",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef8007f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.6260206680169673\n",
      "valid loss: 1.2257428836119946\n",
      "New best score: 1.2257428836119946\n",
      "\n",
      "Epoch 1: train loss: 0.4889146287071611\n",
      "valid loss: 1.1323503856671118\n",
      "New best score: 1.1323503856671118\n",
      "\n",
      "Epoch 2: train loss: 0.443328338787629\n",
      "valid loss: 1.041066593511788\n",
      "New best score: 1.041066593511788\n",
      "\n",
      "Epoch 3: train loss: 0.41811798419646506\n",
      "valid loss: 0.9525380145383029\n",
      "New best score: 0.9525380145383029\n",
      "\n",
      "Epoch 4: train loss: 0.3982787346001976\n",
      "valid loss: 0.872528903087169\n",
      "New best score: 0.872528903087169\n",
      "\n",
      "Epoch 5: train loss: 0.38633111697516537\n",
      "valid loss: 0.8427106081773686\n",
      "New best score: 0.8427106081773686\n",
      "\n",
      "Epoch 6: train loss: 0.3715525363550502\n",
      "valid loss: 0.8132988200540665\n",
      "New best score: 0.8132988200540665\n",
      "\n",
      "Epoch 7: train loss: 0.3626147477298239\n",
      "valid loss: 0.7803101856863487\n",
      "New best score: 0.7803101856863487\n",
      "\n",
      "Epoch 8: train loss: 0.3554087988436138\n",
      "valid loss: 0.7745204644150211\n",
      "New best score: 0.7745204644150211\n",
      "\n",
      "Epoch 9: train loss: 0.3439126586879341\n",
      "valid loss: 0.751159804179103\n",
      "New best score: 0.751159804179103\n",
      "\n",
      "Epoch 10: train loss: 0.3367608132406626\n",
      "valid loss: 0.7307394832698452\n",
      "New best score: 0.7307394832698452\n",
      "\n",
      "Epoch 11: train loss: 0.3349103986451393\n",
      "valid loss: 0.726143489372226\n",
      "New best score: 0.726143489372226\n",
      "\n",
      "Epoch 12: train loss: 0.33255094337260904\n",
      "valid loss: 0.7153456960999524\n",
      "New best score: 0.7153456960999524\n",
      "\n",
      "Epoch 13: train loss: 0.32496147922824303\n",
      "valid loss: 0.7076372661886333\n",
      "New best score: 0.7076372661886333\n",
      "\n",
      "Epoch 14: train loss: 0.3245579350963528\n",
      "valid loss: 0.7137158132076773\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 15: train loss: 0.3223110231571422\n",
      "valid loss: 0.694739673700991\n",
      "New best score: 0.694739673700991\n",
      "\n",
      "Epoch 16: train loss: 0.316797238984289\n",
      "valid loss: 0.7103211336376096\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 17: train loss: 0.31355754684202963\n",
      "valid loss: 0.6851014863458876\n",
      "New best score: 0.6851014863458876\n",
      "\n",
      "Epoch 18: train loss: 0.3133337708458865\n",
      "valid loss: 0.6797808238478668\n",
      "New best score: 0.6797808238478668\n",
      "\n",
      "Epoch 19: train loss: 0.3052925518392295\n",
      "valid loss: 0.664877889862022\n",
      "New best score: 0.664877889862022\n",
      "\n",
      "Epoch 20: train loss: 0.3112154037015516\n",
      "valid loss: 0.6536767141284429\n",
      "New best score: 0.6536767141284429\n",
      "\n",
      "Epoch 21: train loss: 0.30427109464011554\n",
      "valid loss: 0.6474145427735756\n",
      "New best score: 0.6474145427735756\n",
      "\n",
      "Epoch 22: train loss: 0.3019541884212431\n",
      "valid loss: 0.642338418196391\n",
      "New best score: 0.642338418196391\n",
      "\n",
      "Epoch 23: train loss: 0.3003086803845982\n",
      "valid loss: 0.6400922432881798\n",
      "New best score: 0.6400922432881798\n",
      "\n",
      "Epoch 24: train loss: 0.3001959572155562\n",
      "valid loss: 0.6643824893874983\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 25: train loss: 0.2981294698695297\n",
      "valid loss: 0.6285172990973005\n",
      "New best score: 0.6285172990973005\n",
      "\n",
      "Epoch 26: train loss: 0.2958380587907285\n",
      "valid loss: 0.6180560803430388\n",
      "New best score: 0.6180560803430388\n",
      "\n",
      "Epoch 27: train loss: 0.29548542076959194\n",
      "valid loss: 0.6351022618033076\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 28: train loss: 0.2943061939641992\n",
      "valid loss: 0.6177418259337204\n",
      "New best score: 0.6177418259337204\n",
      "\n",
      "Epoch 29: train loss: 0.28994825061312357\n",
      "valid loss: 0.6146590705401487\n",
      "New best score: 0.6146590705401487\n",
      "\n",
      "Epoch 30: train loss: 0.29001624551630467\n",
      "valid loss: 0.6086894985706237\n",
      "New best score: 0.6086894985706237\n",
      "\n",
      "Epoch 31: train loss: 0.28972154075948126\n",
      "valid loss: 0.607470890365456\n",
      "New best score: 0.607470890365456\n",
      "\n",
      "Epoch 32: train loss: 0.2883570367446329\n",
      "valid loss: 0.603868786493465\n",
      "New best score: 0.603868786493465\n",
      "\n",
      "Epoch 33: train loss: 0.2865449856958367\n",
      "valid loss: 0.6114219867096711\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 34: train loss: 0.28812829954031977\n",
      "valid loss: 0.6044985344102287\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 35: train loss: 0.28583189226155664\n",
      "valid loss: 0.5994470754510833\n",
      "New best score: 0.5994470754510833\n",
      "\n",
      "Epoch 36: train loss: 0.2842140240654358\n",
      "valid loss: 0.6218988808728896\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 37: train loss: 0.2874961957243438\n",
      "valid loss: 0.6034724370615394\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 38: train loss: 0.2806195541923465\n",
      "valid loss: 0.5927624536763183\n",
      "New best score: 0.5927624536763183\n",
      "\n",
      "Epoch 39: train loss: 0.28127730846748067\n",
      "valid loss: 0.6016660289477713\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 40: train loss: 0.2867387616604249\n",
      "valid loss: 0.5879656690573001\n",
      "New best score: 0.5879656690573001\n",
      "\n",
      "Epoch 41: train loss: 0.2771380063311596\n",
      "valid loss: 0.590345326899604\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 42: train loss: 0.2797277405406236\n",
      "valid loss: 0.5917401944637695\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 43: train loss: 0.27742373223371014\n",
      "valid loss: 0.5900168710816722\n",
      "3  iterations since best score.\n",
      "\n",
      "Epoch 44: train loss: 0.2783686689497011\n",
      "valid loss: 0.5881938602874488\n",
      "4  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop(model,train_loader,val_loader,optimizer,criterion,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4baa1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005608443, 0.79956955, 0.012906705, 0.033568468, 0.34498852]\n",
      "0.58798647\n"
     ]
    }
   ],
   "source": [
    "losses = per_sample_mse(model, val_loader)\n",
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9855e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fraud reconstruction error: 2.977164\n",
      "Average genuine reconstruction error: 0.22459523\n"
     ]
    }
   ],
   "source": [
    "genuine_losses = np.array(losses)[y_valid.numpy() == 0]\n",
    "fraud_losses = np.array(losses)[y_valid.numpy() == 1]\n",
    "print(\"Average fraud reconstruction error:\", np.mean(fraud_losses))\n",
    "print(\"Average genuine reconstruction error:\", np.mean(genuine_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f88db505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import (average_precision_score, roc_auc_score)\n",
    "\n",
    "# compute AUC-ROC and Average Precision on the validation set by considering the reconstruction errors as predicted fraud scores\n",
    "\n",
    "AUC_ROC = roc_auc_score(y_cv, losses)\n",
    "AP = average_precision_score(y_cv, losses)\n",
    "    \n",
    "performances = pd.DataFrame([[AUC_ROC, AP]], columns=['AUC ROC','Average precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd5bf37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.743221</td>\n",
       "      <td>0.477684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AUC ROC  Average precision\n",
       "0  0.743221           0.477684"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3ccea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at threshold 0.1756 = 0.6274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "thr      = np.percentile(losses, 70)      # e.g. top 30% as “fraud”\n",
    "y_pred   = (losses >= thr).astype(int)\n",
    "recall   = recall_score(y_cv, y_pred)    # binary‐class recall\n",
    "print(f\"Recall at threshold {thr:.4f} = {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
